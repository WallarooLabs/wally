#+LaTeX_CLASS: sendence-com-article-2
#+SETUPFILE: ~/.emacs.dir/org-html-themes/setup/theme-readtheorg.setup
#+TITLE: Docker Topology for Double Divide
#+AUTHOR: Markus Fix
#+EMAIL: markus@sendence.com
#+DATE: 2016-06-16
#+DESCRIPTION: Notes on topology setup for Double Divide using Docker containers
#+KEYWORDS: Sendence, distributed, orchestration, buffy, dagon
#+LANGUAGE: english
#+STARTUP: overview
#+TAGS: PROJECT(p) HOME(h) OFFICE(o) PHONE(t) ERRANDS(e)
#+STARTUP: hidestars
#+LaTeX_CLASS_OPTIONS: [10pt,a4paper,captions=tableheading,headsepline,footsepline]
#+LateX_HEADER: \KOMAoptions{titlepage=true, abstract=true}
#+LaTeX_HEADER: \subtitle{Buffy orchestration}
#+LaTeX_HEADER: \usepackage{paralist}
#+LaTeX_HEADER: \usepackage{amssymb}
#+LaTeX_HEADER: \let\itemize\compactitem
#+LaTeX_HEADER: \let\description\compactdesc
#+LaTeX_HEADER: \let\enumerate\compactenum
#+LaTeX_CLASS_OPTIONS: [captions=tableheading]
#+LATEX: 
#+LATEX: \listoffigures


* Introduction
Dagon can boot the topology using Docker containers. We will use
Double Divide as an example to show how the configuration file entries
correspond to the Docker commands we emit to boot a node.

In this example case our topology has the following nodes:
1. Giles-Sender
2. Leader
3. Giles-Receiver
4. Worker-1
5. Worker-2


* Docker Topology

** Giles-Sender
 #+BEGIN_SRC sh
docker run -u 501 --name giles-sender -e constraint:node==default -h
giles-sender --privileged -i -e LC_ALL=C.UTF-8 -e LANG=C.UTF-8 -v
/bin:/bin:ro -v /lib:/lib:ro -v /lib64:/lib64:ro -v /usr:/usr:ro -v
/Users/fix/projects/Sendence/Buffy/dagon:/Users/fix/projects/Sendence/Buffy/dagon
-w /Users/fix/projects/Sendence/Buffy/dagon --net=buffy
docker.sendence.com:5043/sendence/giles-sender.amd64:sendence-2.3.0-360-g7523161
giles-sender.amd64 --messages=100 --buffy=leader:7000
--file=count-to-hundred.txt --name=giles-sender
--phone_home=10.23.108.54:8080
 #+END_SRC


** Leader
 #+BEGIN_SRC sh
docker run -u 501 --name leader -e constraint:node==default -h leader
--privileged -i -e LC_ALL=C.UTF-8 -e LANG=C.UTF-8 -v /bin:/bin:ro -v
/lib:/lib:ro -v /lib64:/lib64:ro -v /usr:/usr:ro -v
/Users/fix/projects/Sendence/Buffy/dagon:/Users/fix/projects/Sendence/Buffy/dagon
-w /Users/fix/projects/Sendence/Buffy/dagon --net=buffy
docker.sendence.com:5043/sendence/double-divide.amd64:sendence-2.3.0-360-g7523161
double-divide.amd64 --name=leader --metrics=127.0.0.1:9000 -l
--worker_count=2 --leader-data-address=leader:6500
--source=leader:7000 --leader-control-address=leader:6000
--sink=giles-receiver:8000 --phone_home=10.23.108.54:8080
 #+END_SRC


** Giles-Receiver
 #+BEGIN_SRC sh
docker run -u 501 --name giles-receiver -e constraint:node==default -h
giles-receiver --privileged -i -e LC_ALL=C.UTF-8 -e LANG=C.UTF-8 -v
/bin:/bin:ro -v /lib:/lib:ro -v /lib64:/lib64:ro -v /usr:/usr:ro -v
/Users/fix/projects/Sendence/Buffy/dagon:/Users/fix/projects/Sendence/Buffy/dagon
-w /Users/fix/projects/Sendence/Buffy/dagon --net=buffy
docker.sendence.com:5043/sendence/giles-receiver.amd64:sendence-2.3.0-360-g7523161
giles-receiver.amd64 --listen=giles-receiver:8000
--name=giles-receiver --phone_home=10.23.108.54:8080
 #+END_SRC


** Worker-1
 #+BEGIN_SRC sh
docker run -u 501 --name worker-1 -e constraint:node==default -h
worker-1 --privileged -i -e LC_ALL=C.UTF-8 -e LANG=C.UTF-8 -v
/bin:/bin:ro -v /lib:/lib:ro -v /lib64:/lib64:ro -v /usr:/usr:ro -v
/Users/fix/projects/Sendence/Buffy/dagon:/Users/fix/projects/Sendence/Buffy/dagon
-w /Users/fix/projects/Sendence/Buffy/dagon --net=buffy
docker.sendence.com:5043/sendence/double-divide.amd64:sendence-2.3.0-360-g7523161
double-divide.amd64 --source=leader:7000
--leader-control-address=leader:6000 --metrics=127.0.0.1:9000
--leader-data-address=leader:6500 --sink=giles-receiver:8000
--name=worker-1 --phone_home=10.23.108.54:8080
 #+END_SRC



** Worker-2
 docker run -u 501 --name worker-2 -e constraint:node==default -h
 worker-2 --privileged -i -e LC_ALL=C.UTF-8 -e LANG=C.UTF-8 -v
 /bin:/bin:ro -v /lib:/lib:ro -v /lib64:/lib64:ro -v /usr:/usr:ro -v
 /Users/fix/projects/Sendence/Buffy/dagon:/Users/fix/projects/Sendence/Buffy/dagon
 -w /Users/fix/projects/Sendence/Buffy/dagon --net=buffy
 docker.sendence.com:5043/sendence/double-divide.amd64:sendence-2.3.0-360-g7523161
 double-divide.amd64 --source=leader:7000
 --leader-control-address=leader:6000 --metrics=127.0.0.1:9000
 --leader-data-address=leader:6500 --sink=giles-receiver:8000
 --name=worker-2 --phone_home=10.23.108.54:8080

** Dagon Output
 #+BEGIN_SRC sh
 ./dagon --docker=192.168.99.100:2376\
  -t 10 \
  --filepath=docker-example.ini \
  --phone_home=10.23.108.54:8080
 dagon: DOCKER_HOST: 192.168.99.100:2376
 dagon: timeout: 10
 dagon: path: docker-example.ini
 dagon: host: 10.23.108.54
 dagon: service: 8080
 dagon: parse_and_register_container_nodes
 dagon: waited for listener, trying to boot: 1
 dagon: parse_docker_section
 dagon: parse_node_section
 dagon: parse_node_section
 dagon: parse_node_section
 dagon: parse_node_section
 dagon: parse_docker_section
 dagon: parse_node_section
 dagon:   key: docker_network	buffy
 dagon:   key: docker_path	/usr/local/bin/docker
 dagon:   key: docker_repo	docker.sendence.com:5043/sendence/
 dagon: finished registration of nodes
 dagon: listening on 10.23.108.54:8080
 dagon: listener is not ready.
 dagon: listener is not ready.
 dagon: registering leader node: leader
 dagon: registering node: worker-2
 dagon: registering canary node: giles-sender
 dagon: registering node: giles-receiver
 dagon: registering node: worker-1
 dagon: listener is ready!
 dagon: waited for listener, trying to boot: 2
 dagon: cancelling timer
 dagon: listener is ready and nodes are registered. Booting topology.
 dagon: canceled listener timer
 dagon: booting leaders
 dagon: booting leader: leader
 dagon: booting container: leader
 dagon: adding to docker env: DOCKER_CERT_PATH=/Users/fix/.docker/machine/machines/default
 dagon: adding to docker env: DOCKER_TLS_VERIFY=1
 dagon: adding to docker env: DOCKER_MACHINE_NAME=default
 dagon: docker command
 docker run -u 501 --name leader -e constraint:node==default -h leader --privileged -i -e LC_ALL=C.UTF-8 -e LANG=C.UTF-8 -v /bin:/bin:ro -v /lib:/lib:ro -v /lib64:/lib64:ro -v /usr:/usr:ro -v /Users/fix/projects/Sendence/Buffy/dagon:/Users/fix/projects/Sendence/Buffy/dagon -w /Users/fix/projects/Sendence/Buffy/dagon --net=buffy docker.sendence.com:5043/sendence/double-divide.amd64:sendence-2.3.0-360-g7523161 double-divide.amd64 --name=leader --metrics=127.0.0.1:9000 -l --worker_count=2 --leader-data-address=leader:6500 --source=leader:7000 --leader-control-address=leader:6000 --sink=giles-receiver:8000 --phone_home=10.23.108.54:8080
 dagon: timer got canceled
 dagon: leader STDOUT [
	 Using Spike seed 28820116
 **Buffy Leader leader control: leader:6000**

 dagon: leader STDOUT ]
 dagon: leader STDOUT [
	 **Buffy Leader leader data: leader:6500**
** -- Looking for 2 workers --**

dagon: leader STDOUT ]
dagon: connection accepted
dagon: leader STDOUT [
	Source 0: listening on leader:7000
leader control: listening on 172.18.0.2:6000

dagon: leader STDOUT ]
dagon: leader STDOUT [
	leader data: listening on 172.18.0.2:6500
Coordinator: control connection to leader was not set up

dagon: leader STDOUT ]
dagon: leader: Ready
dagon: received ready from child: leader
dagon: leader state: Ready
dagon: leader iscanary: false
dagon: booting workers
dagon: booting worker: giles-receiver
dagon: booting worker: worker-2
dagon: booting worker: worker-1
dagon: booting container: giles-receiver
dagon: adding to docker env: DOCKER_CERT_PATH=/Users/fix/.docker/machine/machines/default
dagon: adding to docker env: DOCKER_TLS_VERIFY=1
dagon: adding to docker env: DOCKER_MACHINE_NAME=default
dagon: docker command
docker run -u 501 --name giles-receiver -e constraint:node==default -h giles-receiver --privileged -i -e LC_ALL=C.UTF-8 -e LANG=C.UTF-8 -v /bin:/bin:ro -v /lib:/lib:ro -v /lib64:/lib64:ro -v /usr:/usr:ro -v /Users/fix/projects/Sendence/Buffy/dagon:/Users/fix/projects/Sendence/Buffy/dagon -w /Users/fix/projects/Sendence/Buffy/dagon --net=buffy docker.sendence.com:5043/sendence/giles-receiver.amd64:sendence-2.3.0-360-g7523161 giles-receiver.amd64 --listen=giles-receiver:8000 --name=giles-receiver --phone_home=10.23.108.54:8080
dagon: booting container: worker-2
dagon: adding to docker env: DOCKER_CERT_PATH=/Users/fix/.docker/machine/machines/default
dagon: adding to docker env: DOCKER_TLS_VERIFY=1
dagon: adding to docker env: DOCKER_MACHINE_NAME=default
dagon: docker command
docker run -u 501 --name worker-2 -e constraint:node==default -h worker-2 --privileged -i -e LC_ALL=C.UTF-8 -e LANG=C.UTF-8 -v /bin:/bin:ro -v /lib:/lib:ro -v /lib64:/lib64:ro -v /usr:/usr:ro -v /Users/fix/projects/Sendence/Buffy/dagon:/Users/fix/projects/Sendence/Buffy/dagon -w /Users/fix/projects/Sendence/Buffy/dagon --net=buffy docker.sendence.com:5043/sendence/double-divide.amd64:sendence-2.3.0-360-g7523161 double-divide.amd64 --source=leader:7000 --leader-control-address=leader:6000 --metrics=127.0.0.1:9000 --leader-data-address=leader:6500 --sink=giles-receiver:8000 --name=worker-2 --phone_home=10.23.108.54:8080
dagon: booting container: worker-1
dagon: adding to docker env: DOCKER_CERT_PATH=/Users/fix/.docker/machine/machines/default
dagon: adding to docker env: DOCKER_TLS_VERIFY=1
dagon: adding to docker env: DOCKER_MACHINE_NAME=default
dagon: docker command
docker run -u 501 --name worker-1 -e constraint:node==default -h worker-1 --privileged -i -e LC_ALL=C.UTF-8 -e LANG=C.UTF-8 -v /bin:/bin:ro -v /lib:/lib:ro -v /lib64:/lib64:ro -v /usr:/usr:ro -v /Users/fix/projects/Sendence/Buffy/dagon:/Users/fix/projects/Sendence/Buffy/dagon -w /Users/fix/projects/Sendence/Buffy/dagon --net=buffy docker.sendence.com:5043/sendence/double-divide.amd64:sendence-2.3.0-360-g7523161 double-divide.amd64 --source=leader:7000 --leader-control-address=leader:6000 --metrics=127.0.0.1:9000 --leader-data-address=leader:6500 --sink=giles-receiver:8000 --name=worker-1 --phone_home=10.23.108.54:8080
dagon: leader STDOUT [
	Metrics Collector: connection failed.

dagon: leader STDOUT ]
dagon: worker-2 STDOUT [
	Using Spike seed 28820395
**Buffy Worker worker-2**

dagon: worker-2 STDOUT ]
dagon: connection accepted
dagon: worker-2 STDOUT [
	worker-2 control: listening on 127.0.0.1:33229
worker-2 data: listening on 127.0.0.1:38107

dagon: worker-2 STDOUT ]
dagon: leader STDOUT [
	Identified worker worker-2 control channel
Identified worker worker-2 data channel
leader is connected.

dagon: leader STDOUT ]
dagon: worker-2 STDOUT [
	Metrics Collector: connection failed.
worker-2 is connected.

dagon: worker-2 STDOUT ]
dagon: worker-1 STDOUT [
	Using Spike seed 28820457
**Buffy Worker worker-1**

dagon: worker-1 STDOUT ]
dagon: connection accepted
dagon: worker-1 STDOUT [
	worker-1 control: listening on 127.0.0.1:44595
worker-1 data: listening on 127.0.0.1:43801

dagon: worker-1 STDOUT ]
dagon: worker-1 STDOUT [
	Metrics Collector: connection failed.
worker-1 is connected.

dagon: worker-1 STDOUT ]
dagon: leader STDOUT [
	Identified worker worker-1 control channel
Identified worker worker-1 data channel

dagon: leader STDOUT ]
dagon: leader STDOUT [
	_--- All worker data channels accounted for! ---_
_--- All worker control channels accounted for! ---_

dagon: leader STDOUT ]
dagon: leader STDOUT [
	leader is connected.

dagon: leader STDOUT ]
dagon: worker-2 STDOUT [
	Acking connections finished!
worker-2 is connected.

dagon: worker-2 STDOUT ]
dagon: worker-1 STDOUT [
	Acking connections finished!
worker-1 is connected.

dagon: worker-1 STDOUT ]
dagon: leader STDOUT [
	_--- All worker interconnections complete! ---_
Spinning up computation **** on node 'leader'

dagon: leader STDOUT ]
dagon: leader STDOUT [
	Spinning up computation **** on node 'worker-2'
Spinning up computation **** on node 'worker-1'

dagon: leader STDOUT ]
dagon: leader STDOUT [
	Spinning up sink on node leader

dagon: leader STDOUT ]
dagon: worker-1 STDERR [
	worker-1 is spinning up a step!

dagon: worker-1 STDERR ]
dagon: worker-1 STDERR [
	worker-1 is spinning up a proxy!

dagon: worker-1 STDERR ]
dagon: worker-1 STDOUT [
	Acking initialization messages finished!

dagon: worker-1 STDOUT ]
dagon: worker-2 STDERR [
	worker-2 is spinning up a step!

dagon: worker-2 STDERR ]
dagon: worker-2 STDERR [
	worker-2 is spinning up a proxy!

dagon: worker-2 STDERR ]
dagon: worker-2 STDOUT [
	Acking initialization messages finished!

dagon: worker-2 STDOUT ]
dagon: leader STDOUT [
	_--- Topology successfully initialized ---_

dagon: leader STDOUT ]
dagon: leader: TopologyReady
dagon: received topology ready from: leader
dagon: booting canary nodes
dagon: booting canary: giles-sender
dagon: booting container: giles-sender
dagon: adding to docker env: DOCKER_CERT_PATH=/Users/fix/.docker/machine/machines/default
dagon: adding to docker env: DOCKER_TLS_VERIFY=1
dagon: adding to docker env: DOCKER_MACHINE_NAME=default
dagon: docker command
docker run -u 501 --name giles-sender -e constraint:node==default -h giles-sender --privileged -i -e LC_ALL=C.UTF-8 -e LANG=C.UTF-8 -v /bin:/bin:ro -v /lib:/lib:ro -v /lib64:/lib64:ro -v /usr:/usr:ro -v /Users/fix/projects/Sendence/Buffy/dagon:/Users/fix/projects/Sendence/Buffy/dagon -w /Users/fix/projects/Sendence/Buffy/dagon --net=buffy docker.sendence.com:5043/sendence/giles-sender.amd64:sendence-2.3.0-360-g7523161 giles-sender.amd64 --messages=100 --buffy=leader:7000 --file=count-to-hundred.txt --name=giles-sender --phone_home=10.23.108.54:8080
dagon: connection accepted
dagon: giles-receiver STDOUT [
	Listening for data

dagon: giles-receiver STDOUT ]
dagon: giles-receiver: Ready
dagon: received ready from child: giles-receiver
dagon: giles-receiver state: Ready
dagon: giles-receiver iscanary: false
dagon: connection accepted
dagon: giles-sender: Ready
dagon: received ready from child: giles-sender
dagon: giles-sender state: Ready
dagon: giles-sender iscanary: true
dagon: starting canary node: giles-sender
dagon: sending start to child: giles-sender
dagon: giles-sender: DoneShutdown
dagon: received done_shutdown from child: giles-sender
dagon: canary reported DoneShutdown ---------------------
dagon: waiting for processing to finish
dagon: wait for processing to finish: 1
dagon: server closed
dagon: leader STDOUT [
	Source 0: server closed

dagon: leader STDOUT ]
dagon: giles-sender exited with exit code: 0
dagon: exited child: giles-sender
dagon: wait for processing to finish: 2
dagon: wait for processing to finish: 3
dagon: wait for processing to finish: 4
dagon: wait for processing to finish: 5
dagon: wait for processing to finish: 6
dagon: wait for processing to finish: 7
dagon: wait for processing to finish: 8
dagon: wait for processing to finish: 9
dagon: wait for processing to finish: 10
dagon: wait for processing to finish: 11
dagon: shutting down topology
dagon: sending shutdown to giles-sender
dagon: sending shutdown to leader
dagon: sending shutdown to giles-receiver
dagon: sending shutdown to worker-2
dagon: don't have a connection to send shutdown to worker-2
dagon: sending shutdown to worker-1
dagon: don't have a connection to send shutdown to worker-1
dagon: leader: DoneShutdown
dagon: server closed
dagon: received done_shutdown from child: leader
dagon: leader STDOUT [
	leader: server closed

dagon: leader STDOUT ]
dagon: worker-1 STDOUT [
	worker-1: server closed

dagon: worker-1 STDOUT ]
dagon: worker-1 STDOUT [
	worker-1: server closed
DataReceiverNotify: closed!

dagon: worker-1 STDOUT ]
dagon: worker-1: DoneShutdown
dagon: received done_shutdown from child: worker-1
dagon: server closed
dagon: worker-2 STDOUT [
	worker-2: server closed

dagon: worker-2 STDOUT ]
dagon: worker-2 STDOUT [
	worker-2: server closed
DataReceiverNotify: closed!

dagon: worker-2 STDOUT ]
dagon: worker-2 STDOUT [
	worker-2: server closed
worker-2: server closed

dagon: worker-2 STDOUT ]
dagon: worker-2: DoneShutdown
dagon: received done_shutdown from child: worker-2
dagon: server closed
dagon: worker-1 STDOUT [
	worker-1: server closed

dagon: worker-1 STDOUT ]
dagon: worker-1 STDOUT [
	worker-1: server closed

dagon: worker-1 STDOUT ]
dagon: leader STDOUT [
	leader: server closed
leader: server closed

dagon: leader STDOUT ]
dagon: leader STDOUT [
	DataReceiverNotify: closed!
leader: server closed

dagon: leader STDOUT ]
dagon: leader STDOUT [
	leader: server closed

dagon: leader STDOUT ]
dagon: worker-1 STDOUT [
	worker-1: server closed

dagon: worker-1 STDOUT ]
dagon: leader STDOUT [
	DataReceiverNotify: closed!

dagon: leader STDOUT ]
dagon: worker-1 STDOUT [
	DataReceiverNotify: closed!

dagon: worker-1 STDOUT ]
dagon: worker-2 STDOUT [
	worker-2: server closed
DataReceiverNotify: closed!

dagon: worker-2 STDOUT ]
dagon: giles-receiver: DoneShutdown
dagon: received done_shutdown from child: giles-receiver
dagon: server closed
dagon: leader exited with exit code: 0
dagon: exited child: leader
dagon: shutting down listener
dagon: worker-2 exited with exit code: 0
dagon: exited child: worker-2
dagon: worker-1 exited with exit code: 0
dagon: exited child: worker-1
dagon: giles-receiver exited with exit code: 0
dagon: exited child: giles-receiver

#+END_SRC

* Process Topology
** double-divide.ini
 #+BEGIN_SRC ini
[giles-sender]
buffy = 127.0.0.1:7000
messages = 10000
sender = true
path = ./giles/sender/sender
name = giles-sender

[giles-receiver]
path = ./giles/receiver/receiver
name = giles-receiver
listen = 127.0.0.1:8000

[leader]
path = ./apps/double-divide/double-divide
leader-control-address = 127.0.0.1:6000
leader-data-address = 127.0.0.1:6500
source = 127.0.0.1:7000
sink = 127.0.0.1:8000
metrics = 127.0.0.1:9000
leader = true
worker_count = 2
name = leader

[worker-1]
path = ./apps/double-divide/double-divide
leader-control-address = 127.0.0.1:6000
leader-data-address = 127.0.0.1:6500
source = 127.0.0.1:7000
sink = 127.0.0.1:8000
metrics = 127.0.0.1:9000
name = worker-1

[worker-2]
path = ./apps/double-divide/double-divide
leader-control-address = 127.0.0.1:6000
leader-data-address = 127.0.0.1:6500
source = 127.0.0.1:7000
sink = 127.0.0.1:8000
metrics = 127.0.0.1:9000
name = worker-2
 #+END_SRC
** Dagon Ouput
#+BEGIN_SRC sh
dagon -t 10 --filepath=double-divide.ini --phone_home=127.0.0.1:8080
dagon: no DOCKER_HOST defined, using processes.
dagon: timeout: 10
dagon: path: double-divide.ini
dagon: host: 127.0.0.1
dagon: service: 8080
dagon: parse_and_register_processes
dagon: waited for listener, trying to boot: 1
dagon: finished registration of nodes
dagon: listener is not ready.
dagon: listener is not ready.
dagon: registering canary node: giles-sender
dagon: registering node: giles-receiver
dagon: registering leader node: leader
dagon: registering node: worker-1
dagon: registering node: worker-2
dagon: listening on 127.0.0.1:8080
dagon: listener is ready!
dagon: waited for listener, trying to boot: 2
dagon: cancelling timer
dagon: listener is ready and nodes are registered. Booting topology.
dagon: canceled listener timer
dagon: booting leaders
dagon: booting leader: leader
dagon: booting process: leader
dagon: leader arg: leader
dagon: leader arg: --source=127.0.0.1:7000
dagon: leader arg: --metrics=127.0.0.1:9000
dagon: leader arg: --leader-control-address=127.0.0.1:6000
dagon: leader arg: -l
dagon: leader arg: --leader-data-address=127.0.0.1:6500
dagon: leader arg: --sink=127.0.0.1:8000
dagon: leader arg: --worker_count=2
dagon: leader arg: --name=leader
dagon: leader arg: --phone_home=127.0.0.1:8080
dagon: timer got canceled
dagon: leader STDOUT [
	Using Spike seed 236429502

dagon: leader STDOUT ]
dagon: leader STDOUT [
	**Buffy Leader leader control: 127.0.0.1:6000**
**Buffy Leader leader data: 127.0.0.1:6500**
** -- Looking for 2 workers --**
leader control: listening on 127.0.0.1:6000
Source 0: listening on 127.0.0.1:7000

dagon: leader STDOUT ]
dagon: leader STDOUT [
	Coordinator: control connection to leader was not set up

dagon: leader STDOUT ]
dagon: connection accepted
dagon: leader STDOUT [
	Metrics Collector: connection failed.
leader data: listening on 127.0.0.1:6500

dagon: leader STDOUT ]
dagon: leader: Ready
dagon: received ready from child: leader
dagon: leader state: Ready
dagon: leader iscanary: false
dagon: booting workers
dagon: booting worker: giles-receiver
dagon: booting worker: worker-1
dagon: booting worker: worker-2
dagon: booting process: giles-receiver
dagon: giles-receiver arg: giles-receiver
dagon: giles-receiver arg: --listen=127.0.0.1:8000
dagon: giles-receiver arg: --name=giles-receiver
dagon: giles-receiver arg: --phone_home=127.0.0.1:8080
dagon: booting process: worker-1
dagon: worker-1 arg: worker-1
dagon: worker-1 arg: --source=127.0.0.1:7000
dagon: worker-1 arg: --metrics=127.0.0.1:9000
dagon: worker-1 arg: --leader-control-address=127.0.0.1:6000
dagon: worker-1 arg: --leader-data-address=127.0.0.1:6500
dagon: worker-1 arg: --sink=127.0.0.1:8000
dagon: worker-1 arg: --name=worker-1
dagon: worker-1 arg: --phone_home=127.0.0.1:8080
dagon: booting process: worker-2
dagon: worker-2 arg: worker-2
dagon: worker-2 arg: --source=127.0.0.1:7000
dagon: worker-2 arg: --metrics=127.0.0.1:9000
dagon: worker-2 arg: --leader-control-address=127.0.0.1:6000
dagon: worker-2 arg: --leader-data-address=127.0.0.1:6500
dagon: worker-2 arg: --sink=127.0.0.1:8000
dagon: worker-2 arg: --name=worker-2
dagon: worker-2 arg: --phone_home=127.0.0.1:8080
dagon: worker-2 STDOUT [
	Using Spike seed 236429509
**Buffy Worker worker-2**

dagon: worker-2 STDOUT ]
dagon: worker-1 STDOUT [
	Using Spike seed 236429508
**Buffy Worker worker-1**

dagon: worker-1 STDOUT ]
dagon: worker-2 STDOUT [
	Metrics Collector: connection failed.
worker-2 is connected.

dagon: worker-2 STDOUT ]
dagon: worker-1 STDOUT [
	Metrics Collector: connection failed.
worker-1 is connected.

dagon: worker-1 STDOUT ]
dagon: connection accepted
dagon: connection accepted
dagon: worker-2 STDOUT [
	worker-2 control: listening on 127.0.0.1:62874
worker-2 data: listening on 127.0.0.1:62875

dagon: worker-2 STDOUT ]
dagon: worker-1 STDOUT [
	worker-1 control: listening on 127.0.0.1:62876

dagon: worker-1 STDOUT ]
dagon: leader STDOUT [
	Identified worker worker-2 control channel
Identified worker worker-1 control channel
Identified worker worker-2 data channel
_--- All worker control channels accounted for! ---_

dagon: leader STDOUT ]
dagon: giles-receiver STDOUT [
	Listening for data

dagon: giles-receiver STDOUT ]
dagon: worker-2 STDOUT [
	Acking connections finished!
worker-2 is connected.

dagon: worker-2 STDOUT ]
dagon: worker-1 STDOUT [
	worker-1 data: listening on 127.0.0.1:62877

dagon: worker-1 STDOUT ]
dagon: leader STDOUT [
	leader is connected.
Identified worker worker-1 data channel
_--- All worker data channels accounted for! ---_
leader is connected.

dagon: leader STDOUT ]
dagon: connection accepted
dagon: giles-receiver: Ready
dagon: received ready from child: giles-receiver
dagon: giles-receiver state: Ready
dagon: giles-receiver iscanary: false
dagon: worker-1 STDOUT [
	Acking connections finished!

dagon: worker-1 STDOUT ]
dagon: leader STDOUT [
	_--- All worker interconnections complete! ---_

dagon: leader STDOUT ]
dagon: worker-1 STDOUT [
	worker-1 is connected.

dagon: worker-1 STDOUT ]
dagon: leader STDOUT [
	Spinning up computation **** on node 'leader'

dagon: leader STDOUT ]
dagon: leader STDOUT [
	Spinning up computation **** on node 'worker-2'

dagon: leader STDOUT ]
dagon: leader STDOUT [
	Spinning up computation **** on node 'worker-1'

dagon: leader STDOUT ]
dagon: leader STDOUT [
	Spinning up sink on node leader

dagon: leader STDOUT ]
dagon: worker-1 STDERR [
	worker-1 is spinning up a step!

dagon: worker-1 STDERR ]
dagon: worker-1 STDERR [
	worker-1 is spinning up a proxy!

dagon: worker-1 STDERR ]
dagon: worker-1 STDOUT [
	Acking initialization messages finished!

dagon: worker-1 STDOUT ]
dagon: worker-2 STDERR [
	worker-2 is spinning up a step!

dagon: worker-2 STDERR ]
dagon: worker-2 STDERR [
	worker-2 is spinning up a proxy!

dagon: worker-2 STDERR ]
dagon: worker-2 STDOUT [
	Acking initialization messages finished!

dagon: worker-2 STDOUT ]
dagon: leader STDOUT [
	_--- Topology successfully initialized ---_

dagon: leader STDOUT ]
dagon: leader: TopologyReady
dagon: received topology ready from: leader
dagon: booting canary nodes
dagon: booting canary: giles-sender
dagon: booting process: giles-sender
dagon: giles-sender arg: giles-sender
dagon: giles-sender arg: --messages=100
dagon: giles-sender arg: --file=./count-to-hundred.txt
dagon: giles-sender arg: --buffy=127.0.0.1:7000
dagon: giles-sender arg: --name=giles-sender
dagon: giles-sender arg: --phone_home=127.0.0.1:8080
dagon: connection accepted
dagon: giles-sender: Ready
dagon: received ready from child: giles-sender
dagon: giles-sender state: Ready
dagon: giles-sender iscanary: true
dagon: starting canary node: giles-sender
dagon: sending start to child: giles-sender
dagon: leader STDOUT [
	>>>>1<<<<

dagon: leader STDOUT ]
dagon: giles-sender: DoneShutdown
dagon: server closed
dagon: received done_shutdown from child: giles-sender
dagon: canary reported DoneShutdown ---------------------
dagon: waiting for processing to finish
dagon: wait for processing to finish: 1
dagon: leader STDOUT [
	>>>>2<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>3<<<<

dagon: leader STDOUT ]
dagon: giles-sender exited with exit code: 0
dagon: exited child: giles-sender
dagon: leader STDOUT [
	>>>>4<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	Source 0: server closed>>>>5<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [


dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>6<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>7<<<<
>>>>8<<<<
>>>>9<<<<
>>>>10<<<<
>>>>11<<<<
>>>>12<<<<
>>>>13<<<<
>>>>14<<<<
>>>>15<<<<
>>>>16<<<<
>>>>17<<<<
>>>>18<<<<
>>>>19<<<<
>>>>20<<<<
>>>>21<<<<
>>>>22<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>23<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>24<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>25<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>26<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>27<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>28<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>29<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>30<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>31<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>32<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>33<<<<
>>>>34<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>35<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>36<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>37<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>38<<<<
>>>>39<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>40<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>41<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>42<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>43<<<<
>>>>44<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>45<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>46<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>47<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>48<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>49<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>50<<<<
>>>>51<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>52<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>53<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>54<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>55<<<<
>>>>56<<<<
>>>>57<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>58<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>59<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>60<<<<
>>>>61<<<<
>>>>62<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>63<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>64<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>65<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>66<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>67<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>68<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>69<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>70<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>71<<<<
>>>>72<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>73<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>74<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>75<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>76<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>77<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>78<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>79<<<<
>>>>80<<<<
>>>>81<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>82<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>83<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>84<<<<
>>>>85<<<<
>>>>86<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>87<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>88<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>89<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>90<<<<
>>>>91<<<<
>>>>92<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>93<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>94<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>95<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>96<<<<
>>>>97<<<<
>>>>98<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>99<<<<

dagon: leader STDOUT ]
dagon: leader STDOUT [
	>>>>100<<<<

dagon: leader STDOUT ]
dagon: wait for processing to finish: 2
dagon: wait for processing to finish: 3
dagon: wait for processing to finish: 4
dagon: wait for processing to finish: 5
dagon: wait for processing to finish: 6
dagon: wait for processing to finish: 7
dagon: wait for processing to finish: 8
dagon: wait for processing to finish: 9
dagon: wait for processing to finish: 10
dagon: wait for processing to finish: 11
dagon: shutting down topology
dagon: sending shutdown to giles-sender
dagon: sending shutdown to leader
dagon: sending shutdown to giles-receiver
dagon: sending shutdown to worker-1
dagon: don't have a connection to send shutdown to worker-1
dagon: sending shutdown to worker-2
dagon: don't have a connection to send shutdown to worker-2
dagon: leader: DoneShutdown
dagon: server closed
dagon: received done_shutdown from child: leader
dagon: worker-2 STDOUT [
	worker-2: server closed
worker-2: server closed

dagon: worker-2 STDOUT ]
dagon: worker-1 STDOUT [
	worker-1: server closed
worker-1: server closed

dagon: worker-1 STDOUT ]
dagon: leader STDOUT [
	leader: server closed
leader: server closed
leader: server closed

dagon: leader STDOUT ]
dagon: leader STDOUT [
	leader: server closed
DataReceiverNotify: closed!
DataReceiverNotify: closed!

dagon: leader STDOUT ]
dagon: leader STDOUT [
	leader: server closed

dagon: leader STDOUT ]
dagon: worker-2: DoneShutdown
dagon: server closed
dagon: received done_shutdown from child: worker-2
dagon: worker-1: DoneShutdown
dagon: server closed
dagon: received done_shutdown from child: worker-1
dagon: worker-1 STDOUT [
	worker-1: server closed
DataReceiverNotify: closed!
worker-1: server closed
worker-1: server closed

dagon: worker-1 STDOUT ]
dagon: worker-2 STDOUT [
	DataReceiverNotify: closed!
worker-2: server closed
worker-2: server closed
worker-2: server closed

dagon: worker-2 STDOUT ]
dagon: worker-2 STDOUT [
	DataReceiverNotify: closed!

dagon: worker-2 STDOUT ]
dagon: worker-1 STDOUT [
	DataReceiverNotify: closed!

dagon: worker-1 STDOUT ]
dagon: worker-1 STDERR [
	Assertion failed: (0), function pony_asio_event_unsubscribe, file src/libponyrt/asio/kqueue.c, line 232.

dagon: worker-1 STDERR ]
dagon: giles-receiver: DoneShutdown
dagon: server closed
dagon: received done_shutdown from child: giles-receiver
dagon: leader STDOUT [
	sink: server closed

dagon: leader STDOUT ]
dagon: worker-2 exited with exit code: 0
dagon: exited child: worker-2
dagon: giles-receiver exited with exit code: 0
dagon: exited child: giles-receiver
dagon: leader exited with exit code: 0
dagon: exited child: leader
dagon: shutting down listener
dagon: worker-1 exited with exit code: 0
dagon: exited child: worker-1
#+END_SRC

* Dagon and AWS
Our current setup does *not* use TLS to secure the Docker communication.
** Boot AWS Cluster
Make sure your AWS credentials are configured.

#+BEGIN_SRC sh
# checkout buffy
cd buffy/orchestration/terraform
make cluster
#+END_SRC

** Reverse SSH Tunnel
Login to the leader node and create a reverse SSH tunnel. The nodes of
our topology will connect back to Dagon through the tunnel.

#+BEGIN_SRC sh
# ssh -i <pem for AWS access> -f -N -T \
#  -R <remote port>:localhost:<local port> ubuntu@<leader-node-ip>
ssh  -i ~/.ssh/ec2/us-east-1.pem -f -N -T -R8080:localhost:8080 ubuntu@52.87.211.240
#+END_SRC
Open a new shell window and continue there. 

** Build and Publish Containers
Point your =DOCKER_HOST= env variable to the AWS swarm on port 2378.
#+BEGIN_SRC sh
export DOCKER_HOST=52.87.211.240:2378
#+END_SRC

You need to be authorized on the Sendence container registry.
#+BEGIN_SRC sh
docker -H tcp://52.87.211.240:2378 login docker.sendence.com:5043
#+END_SRC

Build and publish containers.
#+BEGIN_SRC sh
cd buffy
make arch=amd64 debug=true
make arch=amd64 debug=true build-docker
make arch=amd64 debug=true push-docker
#+END_SRC

** Build Dagon
We're running Dagon on our local computer. That means we have to build
a native binary for Dagon.
#+BEGIN_SRC sh
cd buffy/dagon
stable env ponyc
#+END_SRC

** Login 
Login to the Sendence Docker image registry.
#+BEGIN_SRC sh
docker login docker.sendence.com:5043
#+END_SRC

** Create Network
#+BEGIN_SRC sh
docker network create buffy
#+END_SRC

** Dagon Local
#+BEGIN_SRC sh
cd buffy/dagon
# ./dagon --docker=<leader node ip address>:2378  -t 10 \
#  --filepath=docker-foo.ini\
#  --phone-home=<Dagon ip address>:<port>
./dagon --docker=52.87.211.240:2378  -t 10 \
 --filepath=docker-foo.ini\
 --phone-home=10.23.108.87:8080
#+END_SRC

** Dagon as Container on Leader
If we have a Dagon container that contains both the docker binary and
the Dagon binary we can also start Dagon inside a container.

We are connecting to the docker host of the leader, not the swarm docker
host. We will boot Dagon on the leader node. Dagon will then talk to
the leader docker host (not Swarm) and boot all components defined in
the Dagon config file on the leader.


Point your =DOCKER_HOST= env variable to the AWS leader on port 2375.
#+BEGIN_SRC sh
export DOCKER_HOST=54.173.155.42:2375
#+END_SRC

First we need to copy the configuration file to the docker host.
#+BEGIN_SRC sh
scp -i ~/.ssh/ec2/us-east-1.pem docker-double-divide-aws-leader.ini ubuntu@54.173.155.42:/tmp
#+END_SRC

Then we create a network. We will use the same network in our
configuration for Dagon.
#+BEGIN_SRC sh
docker network create buffy-leader
docker network ls
#+END_SRC

Then we start the Dagon container. Make sure you tell Dagon how to
contact the docker host on it's internal AWS address.

#+BEGIN_SRC sh
docker run --rm -u 0 \
 --name dagon \
 -h dagon \
 --privileged \
 -i \
 -e LC_ALL=C.UTF-8 -e LANG=C.UTF-8 \
 -v /usr/bin:/usr/bin:ro \
 -v /var/run/docker.sock:/var/run/docker.sock \
 -v /bin:/bin:ro \
 -v /lib:/lib:ro \
 -v /lib64:/lib64:ro \
 -v /usr:/usr:ro \
 -v /tmp:/tmp \
 -w /tmp \
 --net=buffy-leader \
 docker.sendence.com:5043/sendence/dagon.amd64:sendence-2.3.0-572-g657b1b3 \
 dagon.amd64 \
 --docker=tcp://10.0.50.3:2375  -t 10 \
 --filepath=/tmp/docker-double-divide-aws-leader.ini \
 --phone-home=dagon:8080
#+END_SRC

Notice that we instruct Dagon to talk to the Swarm master on port 2378
to boot the containers into the Swarm.

** Dagon as Container on Swarm
If we have a Dagon container that contains both the docker binary and
the Dagon binary we can also start Dagon inside a container.

We are connecting to the docker host of the leader on port 2375 , not the swarm docker
host. We will boot Dagon on the leader node. Dagon will then talk to
the Swarm on port 2378 and boot all components defined in the Dagon config file.

Point your =DOCKER_HOST= env variable to the AWS leader on port 2375.
#+BEGIN_SRC sh
export DOCKER_HOST=54.173.155.42:2375
#+END_SRC

First we need to copy the configuration file to the docker host.
#+BEGIN_SRC sh
scp -i ~/.ssh/ec2/us-east-1.pem docker-double-divide-aws-swarm.ini ubuntu@54.173.155.42:/tmp
#+END_SRC

Then we create a network. We will use the same network in our
configuration for Dagon.
#+BEGIN_SRC sh
docker network create buffy-swarm
docker network ls
#+END_SRC

Then we start the Dagon container.
#+BEGIN_SRC sh
docker run --rm -u 0 \
 --name dagon \
 -h dagon \
 --privileged \
 -i \
 -e LC_ALL=C.UTF-8 -e LANG=C.UTF-8 \
 -v /usr/bin:/usr/bin:ro \
 -v /var/run/docker.sock:/var/run/docker.sock \
 -v /bin:/bin:ro \
 -v /lib:/lib:ro \
 -v /lib64:/lib64:ro \
 -v /usr:/usr:ro \
 -v /tmp:/tmp \
 -w /tmp \
 --net=buffy-swarm \
 docker.sendence.com:5043/sendence/dagon.amd64:sendence-2.3.0-572-g657b1b3 \
 dagon.amd64 \
 --docker=tcp://10.0.50.3:2378  -t 10 \
 --filepath=/tmp/docker-double-divide-aws-swarm.ini \
 --phone-home=dagon:8080
#+END_SRC

Notice that we instruct Dagon to talk to the Swarm master on it's EC2
internal address and port 2378 to boot the containers into the Swarm.

** Check IP Tables
Check iptables setup on the docker host (the leader).
#+BEGIN_SRC sh
ssh  -i ~/.ssh/ec2/us-east-1.pem ubuntu@52.87.211.240
sudo iptables -L -n
#+END_SRC
* Dagon and Vagrant Cluster

** Prepare Nodes
Boot the Vagrant cluster using instructions in
=buffy/orchestration/vagrant=.

** Build and Push
Make sure your =DOCKER_HOST= shell environment is pointing to your
local Docker host. Build the AMD64 binaries.
#+BEGIN_SRC sh
DOCKER_HOST=tcp://192.168.99.100:2376 # change this as needed
cd buffy
make clean;make clean
make arch=amd64 debug=true
#+END_SRC

Make sure your =DOCKER_HOST= shell environment is pointing to the
=buffy-leader-1= host. Build containers and push them to the docker
host.
#+BEGIN_SRC sh
export DOCKER_HOST=tcp://10.0.1.200:2375
make arch=amd64 debug=true build-docker
#+END_SRC

** Update Configurations
You'll need to update the image tag referenced in the configuration
files.

** Copy Files
Copy the config files to the =vagrant= directory.
#+BEGIN_SRC sh
cd orchestration/vagrant
cp ../../dagon/docker-double-divide-aws-*.ini ./
#+END_SRC

Login to the leader node and copy the configurations to /tmp.
#+BEGIN_SRC sh
vagrant ssh buffy-leader-1
cd /tmp
cp /vagrant/docker-double-divide-aws-*.ini ./
#+END_SRC

** Boot Dagon Container


* Dipin's AWS Setup
We are mostly working with spot EC2 instances. 

Do not run "make destroy" after your EC2 spot instances have been
killed by AWS. This would boot new nodes and immediately destroy
them. Just run "make cluster" again.

** Prepare environment
On your Mac (in your virtualenv for Ansible) do the following:
Re-create Ansible environment.
#+BEGIN_SRC sh
workon ansible
pip uninstall ansible
pip uninstall boto
pip install -Iv ansible==2.0.0.2
pip install -Iv boto==2.39.0
#+END_SRC

Create cluster
#+BEGIN_SRC sh
make cluster mem_required=16 cpus_required=8
#+END_SRC

Sample output:
#+BEGIN_EXAMPLE
PLAY RECAP *********************************************************************
52.201.243.239             : ok=43   changed=23   unreachable=0    failed=0
54.164.109.60              : ok=43   changed=23   unreachable=0    failed=0
54.172.250.155             : ok=44   changed=23   unreachable=0    failed=0
#+END_EXAMPLE

ssh into the failed node to get the network interface name:
#+BEGIN_SRC sh
ssh -i ~/.ssh/ec2/us-east-1.pem ubuntu@54.172.250.155
#+END_SRC

List available interfaces:
#+BEGIN_SRC sh
ifconfig
#+END_SRC

Sample output:
#+BEGIN_EXAMPLE
ens3      Link encap:Ethernet  HWaddr 0a:ac:49:a4:f2:dd
          inet addr:10.0.81.84  Bcast:10.0.95.255  Mask:255.255.224.0
          inet6 addr: fe80::8ac:49ff:fea4:f2dd/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:9001  Metric:1
          RX packets:3216 errors:0 dropped:0 overruns:0 frame:0
          TX packets:716 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:4166742 (4.1 MB)  TX bytes:85539 (85.5 KB)

lo        Link encap:Local Loopback
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:164 errors:0 dropped:0 overruns:0 frame:0
          TX packets:164 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1
          RX bytes:12104 (12.1 KB)  TX bytes:12104 (12.1 KB)
#+END_EXAMPLE

The interface we're interested in is the first Ethernet interface
named =ens3=.

Logout and edit the file
=/buffy/orchestration/ansible/playbooks/aws.yml=. You need to
substitute "ethernet_interface: eth0" with "ethernet_interface: ens3".

Now configure the cluster:
#+BEGIN_SRC sh
make configure
#+END_SRC

Example output:
#+BEGIN_EXAMPLE
PLAY RECAP *********************************************************************
52.207.224.219             : ok=43   changed=23   unreachable=0    failed=0
54.209.210.91              : ok=44   changed=23   unreachable=0    failed=0
54.234.124.61              : ok=43   changed=23   unreachable=0    failed=0

==> Successfully ran ansible playbook for cluster 'example' in region
'us-east-1'!
#+END_EXAMPLE

Login to leader node and check the Docker daemon is running:
#+BEGIN_SRC sh
ssh -i ~/.ssh/ec2/us-east-1.pem ubuntu@54.209.210.91
docker ps -a
exit
#+END_SRC

Check that Docker Swarm is up and running:
#+BEGIN_SRC sh
docker -H tcp://<leader ip address>:2378
#+END_SRC

** Prepare Pony
Get the prerequisites for building Pony on the leader node.
#+BEGIN_SRC sh
sudo apt-get update
sudo apt-get install -y build-essential git zlib1g-dev libncurses5-dev libssl-dev
wget http://llvm.org/releases/3.8.0/clang+llvm-3.8.0-x86_64-linux-gnu-ubuntu-16.04.tar.xz
tar xvf clang*
cd clang*
sudo cp -r * /usr/local/ && cd ..
#+END_SRC

Get, build and install PCRE2.
#+BEGIN_SRC sh
wget ftp://ftp.csx.cam.ac.uk/pub/software/programming/pcre/pcre2-10.21.tar.bz2
tar xvf pcre2-10.21.tar.bz2
cd pcre2-10.21
./configure --prefix=/usr
make
sudo make install
#+END_SRC

Build and install =ponyc=.
#+BEGIN_SRC sh
git clone https://github.com/ponylang/ponyc.git
cd ponyc
make config=release
sudo make install
cd ..
#+END_SRC

Build and install =stable=.
#+BEGIN_SRC sh
git clone https://github.com/jemc/pony-stable
cd pony-stable
make
sudo make install
#+END_SRC

** Prepare Buffy
Building and publishing on the EC2 leader node are only there to make
sure the binaries being run are AWS compatible. This is necessary
because the ponyc optimized binaries created in virtualbox don't fully
work on AWS and are a source of "fun" debugging. We will likely want
to think about creating unoptimized/generic binaries using ponyc for
general compatibility.


Copy your Github credentials and configurations to the leader node.
#+BEGIN_SRC sh
scp -i ~/.ssh/ec2/us-east-1.pem \
 ~/.gitconfig \
 ubuntu@54.172.250.155:~/
#+END_SRC

Copy the Github ssh key and config to the leader node.
#+BEGIN_SRC sh
scp -i ~/.ssh/ec2/us-east-1.pem \
 ~/.ssh/github_rsa* \
 ubuntu@54.172.250.155:~/.ssh
scp -i ~/.ssh/ec2/us-east-1.pem \
 ~/.ssh/config \
 ubuntu@54.172.250.155:~/.ssh
#+END_SRC

Login to the leader node:
#+BEGIN_SRC sh
ssh -i ~/.ssh/ec2/us-east-1.pem ubuntu@54.172.250.155
#+END_SRC

Checkout the Buffy source code:
#+BEGIN_SRC sh
git clone git@github.com:Sendence/buffy.git
#+END_SRC

Login to the Sendence container registry.
#+BEGIN_SRC sh
sudo docker login docker.sendence.com:5043
#+END_SRC


Change into the main Buffy directory and build the binaries and
container images:
#+BEGIN_SRC sh
cd buffy
make arch=amd64 debug=true
make arch=amd64 debug=true build-docker
make arch=amd64 debug=true push-docker
#+END_SRC

List the container images and note down the latest tag:
#+BEGIN_SRC sh
docker images
#+END_SRC

Sample output:
#+BEGIN_EXAMPLE
REPOSITORY                                                  TAG                           IMAGE ID            CREATED              SIZE
docker.sendence.com:5043/sendence/word-count.amd64          sendence-2.3.0-580-g5733702   351ae0c586c4        About a minute ago   2.59 MB
docker.sendence.com:5043/sendence/dagon-child.amd64         sendence-2.3.0-580-g5733702   c4832918d81c        About a minute ago   746 kB
docker.sendence.com:5043/sendence/double-divide.amd64       sendence-2.3.0-580-g5733702   141c77cc0e88        About a minute ago   2.313 MB
docker.sendence.com:5043/sendence/quadruple.amd64           sendence-2.3.0-580-g5733702   6017bc5804ee        About a minute ago   2.402 MB
docker.sendence.com:5043/sendence/market-spread.amd64       sendence-2.3.0-580-g5733702   93915c141be9        About a minute ago   2.804 MB
docker.sendence.com:5043/sendence/avg-of-avgs.amd64         sendence-2.3.0-580-g5733702   872f875be656        About a minute ago   2.342 MB
docker.sendence.com:5043/sendence/wesley-double.amd64       sendence-2.3.0-580-g5733702   f2e695fdca82        About a minute ago   697.9 kB
docker.sendence.com:5043/sendence/state-avg-of-avgs.amd64   sendence-2.3.0-580-g5733702   8a973cba02ce        About a minute ago   2.376 MB
docker.sendence.com:5043/sendence/wesley-identity.amd64     sendence-2.3.0-580-g5733702   ceccf7bc4629        About a minute ago   690.7 kB
docker.sendence.com:5043/sendence/dagon.amd64               sendence-2.3.0-580-g5733702   496cc1f01563        About a minute ago   1.245 MB
docker.sendence.com:5043/sendence/giles-receiver.amd64      sendence-2.3.0-580-g5733702   9c2c744ba15a        About a minute ago   1.025 MB
docker.sendence.com:5043/sendence/giles-sender.amd64        sendence-2.3.0-580-g5733702   fd8276321b8f        About a minute ago   1.104 MB
#+END_EXAMPLE

The most recent tag is =sendence-2.3.0-580-g5733702= in the example above.

Edit file =buffy/dagon/docker-double-divide-aws-swarm.ini= file and
update the docker.tag lines to the new tag published by the last
build.

#+BEGIN_SRC sh
vi buffy/dagon/docker-double-divide-aws-swarm.ini
#+END_SRC

Copy the relevant configuration files to =/tmp= on the leader node:
#+BEGIN_SRC sh
scp -i ~/.ssh/ec2/us-east-1.pem \
 docker-double-divide-aws-swarm.ini \
 ubuntu@54.172.250.155:/tmp
scp -i ~/.ssh/ec2/us-east-1.pem \
 count-to-hundred.txt\
 ubuntu@54.172.250.155:/tmp
#+END_SRC

Create the docker overlay network ((make sure to do this pointing to
leader docker engine [port 2375] and NOT docker swarm [port 2378]):
#+BEGIN_SRC sj
docker network create -d overlay buffy-swarm
docker network ls
#+END_SRC

Sample output:
#+BEGIN_EXAMPLE
NETWORK ID          NAME                DRIVER
c33da36cbdbb        bridge              bridge
2734ba2139e1        buffy-swarm         overlay
58b5a4a94ed0        host                host
a791125ab6c0        none                null
f40bb6e7a756        root_default        bridge
#+END_EXAMPLE

Determine the IP address of the eth0/ens3 interface address for the leader.
#+BEGIN_SRC sh
ifconfig
#+END_SRC


Change the =DOCKER_HOST= variable to point to the Swarm:
#+BEGIN_SRC sh
export DOCKER_HOST=tcp://10.0.79.127:2378
#+END_SRC

Pull all images that you just published onto the Docker Swarm:
#+BEGIN_SRC sh
docker pull docker.sendence.com:5043/sendence/dagon.amd64:sendence-2.3.0-580-g5733702
#+END_SRC

Start Dagon on the Swarm:
#+BEGIN_EXAMPLE
docker --host=tcp://<leader host or external ip>:2378 run -u 0 \
 --name dagon  -h dagon  --privileged  -i  -e LC_ALL=C.UTF-8 -e LANG=C.UTF-8 \
 -e "constraint:node==<leader internal hostname>" -v /usr/bin:/usr/bin:ro  \
 -v /var/run/docker.sock:/var/run/docker.sock  -v /bin:/bin:ro  \
 -v /lib:/lib:ro  -v /lib64:/lib64:ro  -v /usr:/usr:ro  -v /tmp:/tmp  \
 -w /tmp  \
 --net=buffy-swarm  \
 docker.sendence.com:5043/sendence/dagon.amd64:<docker tag>  \
 dagon.amd64  --docker=tcp://<leader internal ip>:2378  -t 10  \
 --filepath=/tmp/docker-double-divide-aws-swarm.ini  --phone-home=dagon:8080
#+END_EXAMPLE

Using the internal IP address of our leader the command above looks
like this:
#+BEGIN_SRC sh
docker --host=tcp://10.0.79.127:2378 run -u 0 \
 --name dagon  -h dagon  --privileged  -i  -e LC_ALL=C.UTF-8 -e LANG=C.UTF-8 \
 -e "constraint:node==ip-10-0-79-127" -v /usr/bin:/usr/bin:ro  \
 -v /var/run/docker.sock:/var/run/docker.sock  -v /bin:/bin:ro  \
 -v /lib:/lib:ro  -v /lib64:/lib64:ro  -v /usr:/usr:ro  -v /tmp:/tmp  \
 -w /tmp  \
 --net=buffy-swarm \
 docker.sendence.com:5043/sendence/dagon.amd64:sendence-2.3.0-580-g5733702 \
 dagon.amd64  --docker=tcp://10.0.79.127:2378  -t 10  \
 --filepath=/tmp/docker-double-divide-aws-swarm.ini  --phone-home=dagon:8080
#+END_SRC

** Dipin's Instructions
The following are the steps:
1. create & configure cluster (with makefile)
2. ssh into leader node
3. git clone buffy repo
4. build binaries using make arch=amd64
5. build/push docker containers using make arch=amd64 push-docker
6. copy relevant files (ini, etc) to /tmp on leader node (use latest
   in repo with edits for image tag as necessary)
7. exit ssh
8. docker network create -d overlay buffy-swarm (make sure to do this
   pointing to leader docker engine [port 2375] and NOT docker swarm
   [port 2378])
9. docker pull all images just pushed (dagon, giles-sender,
   giles-receiver, etc) onto docker swarm (port 2378 on leader)
10.
#+BEGIN_SRC sh
docker --host=tcp://<leader host or external ip>:2378 run -u 0 \
 --name dagon  -h dagon  --privileged  -i  -e LC_ALL=C.UTF-8 -e LANG=C.UTF-8 \
 -e "constraint:node==<leader internal hostname>â€œ -v /usr/bin:/usr/bin:ro  \
 -v /var/run/docker.sock:/var/run/docker.sock  -v /bin:/bin:ro  \
 -v /lib:/lib:ro  -v /lib64:/lib64:ro  -v /usr:/usr:ro  -v /tmp:/tmp  \
 -w /tmp  --net=buffy-swarm  docker.sendence.com:5043/sendence/dagon.amd64:<docker tag>  
 dagon.amd64  --docker=tcp://<leader internal ip>:2378  -t 10  \
 --filepath=/tmp/docker-double-divide-aws-swarm.ini  --phone-home=dagon:8080
#+END_SRC

#+END_SRC

Steps 2 - 7 are only there to make sure the binaries being run are AWS
compatible. This is necessary because the ponycoptimized binaries
created in virtualbox don't fully work on AWS and are a source of
"fun" debugging. We will likely want to think about creating
unoptimized/generic binaries using ponyc for general compatibility.

Step 8 is important because if you create an overlay network pointing
at docker swarm it'll look like it's what's required but it doesn't
function the same as when it is creating pointing at docker engine.


Step 6 is important because there are some minor tweaks to the
constraints (got the syntax correct and had to remove the quotes) in
the ini file for swarm to make sure it works correctly.


Step 9 is important because docker swarm doesn't automagically pull
images like docker engine does.


Step 10 has some placeholders in <> that need to be replaced for dagon
to start in the right location using the right docker swarm instance.

* Demo Quickstart
** Cluster Boot
Checkout the Buffy repository. If your AWS configurtion is up to date
you can then just run the following commands.
#+BEGIN_SRC sh
cd buffy/orchestration/terraform
make cluster mem_required=16 cpus_required=8 no_spot=true
#+END_SRC

Result:
#+BEGIN_SRC sh
PLAY RECAP *********************************************************************
52.87.154.214              : ok=43   changed=23   unreachable=0    failed=0
54.175.144.43              : ok=43   changed=23   unreachable=0    failed=0
54.87.214.83               : ok=44   changed=23   unreachable=0    failed=0
#+END_SRC

Login to the leader node:
#+BEGIN_SRC sh
ssh -i ~/.ssh/ec2/us-east-1.pem ubuntu@54.87.214.83 
#+END_SRC

Create the Docker network:
#+BEGIN_SRC sh


#+END_SRC

Get the internal IP address for the leader node:
#+BEGIN_SRC sh
ifconfig |grep -A 2 ens3
#+END_SRC

Result:
#+BEGIN_EXAMPLE
ens3      Link encap:Ethernet  HWaddr 06:7b:31:68:92:5b
          inet addr:10.0.12.200  Bcast:10.0.31.255  Mask:255.255.224.0
          inet6 addr: fe80::47b:31ff:fe68:925b/64 Scope:Link
#+END_EXAMPLE

Create the docker overlay network ((make sure to do this pointing to
leader docker engine [port 2375] and NOT docker swarm [port 2378]):
#+BEGIN_SRC sj
docker network create -d overlay buffy-swarm
docker network ls
#+END_SRC

Change the =DOCKER_HOST= variable to point to the Swarm:
#+BEGIN_SRC sh
export DOCKER_HOST=tcp://10.0.12.200:2378
#+END_SRC

Pull all images that you just published onto the Docker Swarm:
#+BEGIN_SRC sh
docker pull docker.sendence.com:5043/sendence/dagon.amd64:sendence-2.3.0-580-g5733702
docker pull docker.sendence.com:5043/sendence/word-count.amd64:sendence-2.3.0-580-g5733702
docker pull docker.sendence.com:5043/sendence/dagon-child.amd64:sendence-2.3.0-580-g5733702
docker pull docker.sendence.com:5043/sendence/double-divide.amd64:sendence-2.3.0-580-g5733702
docker pull docker.sendence.com:5043/sendence/quadruple.amd64:sendence-2.3.0-580-g5733702
docker pull docker.sendence.com:5043/sendence/market-spread.amd64:sendence-2.3.0-580-g5733702
docker pull docker.sendence.com:5043/sendence/avg-of-avgs.amd64:sendence-2.3.0-580-g5733702
docker pull docker.sendence.com:5043/sendence/wesley-double.amd64:sendence-2.3.0-580-g5733702
docker pull docker.sendence.com:5043/sendence/state-avg-of-avgs.amd64:sendence-2.3.0-580-g5733702
docker pull docker.sendence.com:5043/sendence/wesley-identity.amd64:sendence-2.3.0-580-g5733702
docker pull docker.sendence.com:5043/sendence/giles-receiver.amd64:sendence-2.3.0-580-g5733702
docker pull docker.sendence.com:5043/sendence/giles-sender.amd64:sendence-2.3.0-580-g5733702
#+END_SRC

** Copy configurations
On your MacBook again upload the configuration the =/tmp= directory of
the leader node:
#+BEGIN_SRC sh
cd buffy/dagon
scp -i ~/.ssh/ec2/us-east-1.pem \
 docker-double-divide-aws-swarm.ini \
 ubuntu@54.87.214.83:/tmp
scp -i ~/.ssh/ec2/us-east-1.pem \
 count-to-hundred.txt\
 ubuntu@54.87.214.83:/tmp
#+END_SRC

** Swarm Dagon Boot
Boot Dagon on the Swarm. Make sure you use the internal VPC IP address
of the leader node in the =--host= sections and the internal name in
the =constraint= section of the command. You don't want to rely on
name resolving when you boot a container image on one of the worker
nodes.
#+BEGIN_SRC sh
docker --host=tcp://10.0.12.200:2378 run -u 0 \
 --name dagon  -h dagon  --privileged  -i  -e LC_ALL=C.UTF-8 -e LANG=C.UTF-8 \
 -e "constraint:node==ip-10-0-12-200" -v /usr/bin:/usr/bin:ro  \
 -v /var/run/docker.sock:/var/run/docker.sock  -v /bin:/bin:ro  \
 -v /lib:/lib:ro  -v /lib64:/lib64:ro  -v /usr:/usr:ro  -v /tmp:/tmp  \
 -w /tmp  \
 --net=buffy-swarm \
 docker.sendence.com:5043/sendence/dagon.amd64:sendence-2.3.0-580-g5733702 \
 dagon.amd64  --docker=tcp://10.0.12.200:2378  -t 10  \
 --filepath=/tmp/docker-double-divide-aws-swarm.ini  --phone-home=dagon:8080
#+END_SRC

** Leader Dagon Boot
#+BEGIN_SRC sh
docker --host=tcp://10.0.12.200:2375 run -u 0 \
 --name dagon  -h dagon  --privileged  -i  -e LC_ALL=C.UTF-8 -e LANG=C.UTF-8 \
 -e "constraint:node==ip-10-0-12-200" -v /usr/bin:/usr/bin:ro  \
 -v /var/run/docker.sock:/var/run/docker.sock  -v /bin:/bin:ro  \
 -v /lib:/lib:ro  -v /lib64:/lib64:ro  -v /usr:/usr:ro  -v /tmp:/tmp  \
 -w /tmp  \
 --net=buffy-swarm \
 docker.sendence.com:5043/sendence/dagon.amd64:sendence-2.3.0-580-g5733702 \
 dagon.amd64  --docker=tcp://10.0.12.200:2375  -t 10  \
 --filepath=/tmp/docker-double-divide-aws-swarm.ini  --phone-home=dagon:8080
#+END_SRC

* Dagon Docker with Docker VirtualBox
For ease of testing you can run Dagon inside a container using the
default VirtualBox VM that comes with the default Docker install.

** Build
#+BEGIN_SRC sh
cd buffy
make arch=amd64
make arch=amd64 build-docker
#+END_SRC

** Prepare
#+BEGIN_SRC sh
docker images
#+END_SRC
Note down the tag of your most recent build. In our case that's
=sendence-2.3.0-644-g18cccc6=.

Get the address of your local Docker host.
#+BEGIN_SRC sh
docker-machine ls
#+END_SRC

Result:
#+BEGIN_EXAMPLE
NAME      ACTIVE   DRIVER       STATE     URL                         SWARM   DOCKER    ERRORS
default   *        virtualbox   Running   tcp://192.168.99.100:2376           v1.11.2
#+END_EXAMPLE

Create the docker network.
#+BEGIN_SRC sh
docker network create buffy
#+END_SRC

** Run
We are running all containers on our local Docker host VM. In this
case we don't have to copy the configuration file. We can just mount
the local directory and read the file from there. We will also mount
the local directory that contains the pem file to enable TLS
encryption for our connnection to the Docker host.
#+BEGIN_SRC sh
docker --host=tcp://192.168.99.100:2376 run -u 0 \
 --name dagon  -h dagon \
 --privileged  -i \
 -e LC_ALL=C.UTF-8 -e LANG=C.UTF-8 \
 -e "constraint:node==default" -v /usr/bin:/usr/bin:ro \
 -v /var/run/docker.sock:/var/run/docker.sock  -v /bin:/bin:ro \
 -v /lib:/lib:ro  -v /lib64:/lib64:ro  -v /usr:/usr:ro \
 -v /Users/fix/projects/Sendence/Buffy/dagon:/tmp \
 -v /Users/fix/.docker/machine/machines/default/:/certs \
 -w /tmp \
 --net=buffy \
 docker.sendence.com:5043/sendence/dagon.amd64:sendence-2.3.0-644-g18cccc6 \
 dagon.amd64  --docker=tcp://192.168.99.100:2376 \
 --tag=sendence-2.3.0-644-g18cccc6 \
 -t 10 \
 --filepath=/tmp/docker-double-divide.ini \
 --phone-home=dagon:8080
#+END_SRC
